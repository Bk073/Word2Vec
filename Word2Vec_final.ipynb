{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOwJVE1wRbGcuwyl7HiHvM1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bk073/Word2Vec/blob/main/Word2Vec_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5HmoXV23CN6U"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMzW09CAYRBF",
        "outputId": "35294f26-c5b1-4701-caee-1d38de82ed60"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "L50tnoVCCSNm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process data"
      ],
      "metadata": {
        "id": "4WPa9-87E-Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  import codecs\n",
        "  with codecs.open(file_path, 'r', encoding='latin1',errors='ignore') as fdata:\n",
        "    data = fdata.read()\n",
        "  return data\n",
        "        \n",
        "  # with open(file_path, 'r') as f:\n",
        "  #   for line in f:\n",
        "  #     line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
        "  #     content += line\n",
        "  # return content"
      ],
      "metadata": {
        "id": "F4orurpeCUty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 20-news group data\n",
        "# file_path = tf.keras.utils.get_file(fname='wiki_dump', origin='http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2', extract=True, cache_subdir='/content/drive/MyDrive/word2vec')"
      ],
      "metadata": {
        "id": "T_uSz71jCct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tarfile\n",
        "\n",
        "# t_file = tarfile.open('/content/drive/MyDrive/word2vec/20_news_group.tar.gz')\n",
        "# t_file.extractall('/content/drive/MyDrive/word2vec/20_news_group')\n",
        "# t_file.close()"
      ],
      "metadata": {
        "id": "IePdpl8JFUdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_dump(root_path):\n",
        "  train_data_list = ''\n",
        "  for i in os.listdir(root_path):\n",
        "    f = os.path.join(root_path, i)\n",
        "    for j in os.listdir(f):\n",
        "      # train_data_list.append(os.path.join(f, j))\n",
        "      train_data_list += read_file(os.path.join(f, j))\n",
        "      # except:\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "  \n",
        "  return train_data_list"
      ],
      "metadata": {
        "id": "ltDcQo0MCf4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dir = '/content/drive/MyDrive/word2vec/20_news_group/20news-bydate-train'\n",
        "# word_list = data_dump(train_dir)"
      ],
      "metadata": {
        "id": "-z_P2K2VCj9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/word2vec/words.txt', 'w') as f:\n",
        "#   f.write(word_list)"
      ],
      "metadata": {
        "id": "JTA9bGkRYq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_file_path =  '/content/drive/MyDrive/word2vec/words.txt'\n",
        "# test\n",
        "# raw_file_path = '/content/drive/MyDrive/word2vec/text.txt'\n",
        "with open(raw_file_path, 'r') as f:\n",
        "  raw_file = f.read()"
      ],
      "metadata": {
        "id": "7LOMc9NgYsbd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sentence, lang):\n",
        "  # lower = sentence.lower()\n",
        "  # email_removed = tf.strings.regex_replace(\n",
        "  #     lower, \"\\S*@\\S*\\s?\", \"\"\n",
        "  # )\n",
        "  # number_removed = tf.strings.regex_replace(\n",
        "  #     email_removed, \"[0-9]\", ' '\n",
        "  # )\n",
        "  # punctuation_removed =  tf.strings.regex_replace(\n",
        "  #     number_removed, '[%s]' % re.escape(string.punctuation), ' '\n",
        "  # )\n",
        "  # multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed), separator=\" \")\n",
        "  # return multiple_space_removed\n",
        "  if lang == 'eng':\n",
        "    punctuation_ = set(string.punctuation)\n",
        "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'\\S*@\\S*\\s?', ' ', sentence)\n",
        "    sentence = re.sub(r'[0-9]', ' ', sentence)\n",
        "    # sentence = ''.join(ch for ch in sentence if ch not in punctuation_)\n",
        "    # sentence = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *', ' ', sentence)\n",
        "    # sentence = re.sub(r\"[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *\", \" \", sentence)\n",
        "    sentence = sentence.translate(translator)\n",
        "    sentence = re.sub(r'\\n', ' ', sentence)\n",
        "    sentence = re.sub(r'\\t+', ' ', sentence)\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "l2T3R9YeCoWd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocess_text(raw_file, 'eng')\n",
        "print(preprocessed_data[:100])"
      ],
      "metadata": {
        "id": "pKR5ai2-C0QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf7d6c84-f10d-4ec2-f55b-5b51085a137e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from mathew subject alt atheism faq atheist resources summary books addresses music anything related\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subsampling(train_data, threshold=1e-5):\n",
        "  total_words = train_data.split(' ')\n",
        "  print(f\"total words: {len(total_words)}\")\n",
        "  print(f\"unique words: {len(set(total_words))}\")\n",
        "\n",
        "  c = Counter (total_words)\n",
        "  total_counts = len(total_words)\n",
        "  freq = {word: (count / total_counts) for word, count in c.items()}\n",
        "  probability = {word: (1-np.sqrt(threshold/freq[word])) for word in c}\n",
        "\n",
        "  train_words = [word for word in total_words if 0.2 < (1 - probability[word])]\n",
        "  print(f\"subsampled vocab length: {len(train_words)}\")\n",
        "\n",
        "  return train_words, total_words\n"
      ],
      "metadata": {
        "id": "r0HFHNQ0DEv8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words, total_words = subsampling(preprocessed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p45gLUe1n3z7",
        "outputId": "d9e85409-2be4-42c5-f080-e5b10bb58410"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words: 3300978\n",
            "unique words: 77710\n",
            "subsampled vocab length: 1245470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syiRzNCDoTyC",
        "outputId": "8087a4e9-e187-407e-a53a-9e5d6044f519"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mathew',\n",
              " 'alt',\n",
              " 'atheism',\n",
              " 'faq',\n",
              " 'atheist',\n",
              " 'resources',\n",
              " 'summary',\n",
              " 'books',\n",
              " 'addresses',\n",
              " 'music',\n",
              " 'related',\n",
              " 'atheism',\n",
              " 'faq',\n",
              " 'atheism',\n",
              " 'books',\n",
              " 'music',\n",
              " 'fiction',\n",
              " 'addresses',\n",
              " 'contacts',\n",
              " 'expires']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.TextLineDataset(raw_file_path)"
      ],
      "metadata": {
        "id": "EQo3vS2tFr_0"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_data.take(2):\n",
        "  print(text.numpy())"
      ],
      "metadata": {
        "id": "gwh2-OjCFtJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bda6850-561d-409a-e2e8-062f09f62660"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'From: mathew <mathew@mantis.co.uk>'\n",
            "b'Subject: Alt.Atheism FAQ: Atheist Resources'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  lower = tf.strings.lower(text)\n",
        "  # remove emails\n",
        "  email_removed = tf.strings.regex_replace(\n",
        "      lower, \"\\S*@\\S*\\s?\", \"\"\n",
        "  )\n",
        "  # remove numbers\n",
        "  number_removed = tf.strings.regex_replace(\n",
        "      email_removed, \"[0-9]\", ' '\n",
        "  )\n",
        "  tab_removed = tf.strings.regex_replace(number_removed, \"\\t+\", \" \")\n",
        "  # remove punctuations\n",
        "  punctuation_removed =  tf.strings.regex_replace(\n",
        "      tab_removed, '[%s]' % re.escape(string.punctuation), ' '\n",
        "  )\n",
        "  # remove multiple blank spaces\n",
        "  # multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed), separator=\" \")\n",
        "  multiple_space_removed = tf.strings.regex_replace(punctuation_removed, ' +', ' ')\n",
        "  return multiple_space_removed\n"
      ],
      "metadata": {
        "id": "jRe_vAsVF92n"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(subsampled_words))\n",
        "sequence_length = 10\n",
        "\n",
        "vectorize_layers = layers.TextVectorization(\n",
        "    standardize=preprocess,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "sw_mGe7UF-k_"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layers.adapt(train_data.batch(1024))\n",
        "# vectorize_layers.adapt(train_words)"
      ],
      "metadata": {
        "id": "rsy6HNQpGFlR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorize_layers.vocabulary_size())\n",
        "print(vectorize_layers.get_vocabulary()[:20])"
      ],
      "metadata": {
        "id": "OoFIpVUQGHne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1b2387-70c4-4400-e95a-8e4a008b27e1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77292\n",
            "['', '[UNK]', 'the', 'to', 'of', 'a', 'ax', 'and', 'i', 'in', 'is', 'that', 'it', 'for', 'you', 's', 'from', 't', 'on', 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[ True if x in subsampled_words else False for x in inverse_vocab]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "A_K7iSsaplHR",
        "outputId": "3f89da8d-95a8-469f-8906-ee577a9c12e1"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-136-639ce2b3ca76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubsampled_words\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minverse_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-136-639ce2b3ca76>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubsampled_words\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minverse_vocab\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = vectorize_layers.get_vocabulary()\n",
        "inverse_vocab[-20:]"
      ],
      "metadata": {
        "id": "js7p0SxAGb5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a8c89ab-4804-4228-9a09-ab8d6b855394"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['affectionately',\n",
              " 'affectionate',\n",
              " 'affection',\n",
              " 'affaire',\n",
              " 'affadavit',\n",
              " 'afew',\n",
              " 'afeiqy',\n",
              " 'afeiqx',\n",
              " 'afeiqvf',\n",
              " 'afeial',\n",
              " 'afeiafei',\n",
              " 'afdzqvg',\n",
              " 'afdu',\n",
              " 'afdod',\n",
              " 'afarensis',\n",
              " 'afal',\n",
              " 'afair',\n",
              " 'afa',\n",
              " 'aether',\n",
              " 'aetana']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vector_ds = train_data.batch(1024).prefetch(AUTOTUNE).map(vectorize_layers).unbatch()"
      ],
      "metadata": {
        "id": "dSrrOXKPGfpg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub sampling\n",
        "for i in text_vector_ds.take(2):\n",
        "  print(f\"Vector: {i.numpy()} => word: {[inverse_vocab[x] for x in i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrslo1nIbVvo",
        "outputId": "1d3c3de5-7218-4f09-ed34-ab7ff1a1c057"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector: [  16 3558    0    0    0    0    0    0    0    0] => word: ['from', 'mathew', '', '', '', '', '', '', '', '']\n",
            "Vector: [  31 1032 1070  503 1597 1480    0    0    0    0] => word: ['subject', 'alt', 'atheism', 'faq', 'atheist', 'resources', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inverse_vocab[3558])\n",
        "\n",
        "if inverse_vocab[3558] in subsampled_words:\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbUU1jwxqZ-w",
        "outputId": "87ab9a32-bfef-4c77-ff59-705debce2f56"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mathew\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "metadata": {
        "id": "BtR7YOGSGidY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9c96774-96ba-46df-f437-49c154bc86c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inverse_vocab[-1] in subsampled_words:\n",
        "  print(\"true\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4StMfqlwnixd",
        "outputId": "d9ebd864-ead8-4b04-b0fb-c6332c22e2a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating vocab"
      ],
      "metadata": {
        "id": "FFyiFmAiD1bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab = Counter(set(total_words))\n",
        "id_word = {id:word for id, word in enumerate(inverse_vocab)}\n",
        "word_id = {word:id for id, word in id_word.items()}"
      ],
      "metadata": {
        "id": "-37C2ndaDzNr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-gram with sub-sampling"
      ],
      "metadata": {
        "id": "7llhhatkEULV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram(index_vocab, context_window):\n",
        "  context_vector = []\n",
        "  # negative_vector = []\n",
        "  try:\n",
        "    padding_index = index_vocab.tolist().index(0)\n",
        "    for index, target in enumerate(index_vocab):\n",
        "      # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "      # TODO\n",
        "      # check if the target's probability < threshold\n",
        "      if (inverse_vocab[target] in subsampled_words):\n",
        "        if (index<padding_index):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < padding_index else padding_index\n",
        "          context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if (c != index and inverse_vocab[index_vocab[c]] in subsampled_words)])\n",
        "        # elif index > padding_index-context_window-1 and index < padding_index:\n",
        "          # left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          # right_index = index+context_window+1 if (index+context_window+1) <= padding_index else len(padding_index)\n",
        "          # context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if c != index])\n",
        "      \n",
        "      else:\n",
        "        continue\n",
        "  except:\n",
        "    for index, target in enumerate(index_vocab):\n",
        "    # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "    # TODO\n",
        "    # Check probability i.e sub-sampling need to be done\n",
        "      if (inverse_vocab[target] in subsampled_words):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < len(index_vocab) else len(index_vocab)\n",
        "          context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if (c != index and inverse_vocab[index_vocab[c]] in subsampled_words)])\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  context_vector.sort()\n",
        "  context_vector = list(k for k, _ in itertools.groupby(context_vector))\n",
        "  return context_vector\n",
        "\n",
        "  #   negative_sample = negative_sampling(target, context_vector, negative_sample_number, vocab, vocab_size)\n",
        "  #   negative_vector.extend(negative_sample)\n",
        "  # return context_vector, negative_vector"
      ],
      "metadata": {
        "id": "zpJ9LxaMETaK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = [31, 1032, 1070, 503, 1597, 1480]\n",
        "window_size=2\n",
        "positive_skip_grams_2 = skip_gram(example_sentence, window_size)\n",
        "print(positive_skip_grams_2)\n",
        "print(len(positive_skip_grams_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMbMB-UlEfV3",
        "outputId": "e70d6575-daeb-459c-c488-8917a075570e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[503, 1032], [503, 1070], [503, 1480], [503, 1597], [1032, 503], [1032, 1070], [1070, 503], [1070, 1032], [1070, 1597], [1480, 503], [1480, 1597], [1597, 503], [1597, 1070], [1597, 1480]]\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inverse_vocab[31] in subsampled_words:\n",
        "  print(\"True\")"
      ],
      "metadata": {
        "id": "QPd2aBXf05o4"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in example_sentence:\n",
        "  if inverse_vocab[31] in subsampled_words:\n",
        "    print(\"True\")"
      ],
      "metadata": {
        "id": "laRB1iaPs8TS"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative sampling"
      ],
      "metadata": {
        "id": "iRTCQHOnEuIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## negative sampling\n",
        "def negative_sampling(target, context, negative_sample_number, vocab_size):\n",
        "  # vocab_size = len(vocab)\n",
        "  # TODO\n",
        "# this implementation continues in loop when context_window is greater\n",
        "# suppose we have vocab size of 4 [1,2,3,4] and context_window = 3 and negative sample = 2\n",
        "# then we can not get a negative sample of size 2 and the while loop will continue forever\n",
        "# this need to be handle\n",
        "  negative_sample = []\n",
        "  # negative_sample_number = 2\n",
        "  # vocab_size = 7\n",
        "  # vocab = index_vocab\n",
        "  # or get the context for particular index and sample negative values from vocab except the context\n",
        "  while len(negative_sample) < negative_sample_number:\n",
        "    import pdb\n",
        "    # pdb.set_trace()\n",
        "    sample = random.randint(0, vocab_size-1)\n",
        "    # if inverse_vocab[sample] in subsampled_words:\n",
        "    if (sample != context) and (sample not in negative_sample) and sample != target:\n",
        "      negative_sample.append(sample)\n",
        "  return tf.convert_to_tensor(negative_sample, dtype='int64')"
      ],
      "metadata": {
        "id": "Fi3JQixXEwbg"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate training data"
      ],
      "metadata": {
        "id": "H5OuUVxHE2mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram_with_negative_sampling(sequences,context_window, negative_sample_number, vocab_size):\n",
        "  targets, contexts, labels = [],[],[]\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "    import pdb\n",
        "    positive_skip_grams = skip_gram(sequence, context_window)\n",
        "    # pdb.set_trace()\n",
        "    if positive_skip_grams:\n",
        "      for target_word, context_word in positive_skip_grams:\n",
        "        context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
        "        negative_sampling_candidate = negative_sampling(target_word, context_word, negative_sample_number, vocab_size)\n",
        "\n",
        "        negative_sampling_candidate = tf.expand_dims(negative_sampling_candidate, 1)\n",
        "        context = tf.concat([context_class, negative_sampling_candidate], 0)\n",
        "        label = tf.constant([1] + [0]*negative_sample_number, dtype=\"int64\")\n",
        "\n",
        "        targets.append(target_word)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "    else:\n",
        "      continue\n",
        "  \n",
        "  return targets, contexts, labels\n"
      ],
      "metadata": {
        "id": "MpEqIvgyE1qD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sampled training words to int\n",
        "\n",
        "training_word_int = [word_id[word] for word in train_words]\n",
        "training_word_int[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "WpRXzbnwEDPg",
        "outputId": "7daefd47-65f0-495e-b426-39dcd1c6a147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c9cbbd606e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mapping sampled training words to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_word_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_word_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets, contexts, labels = skip_gram_with_negative_sampling(\n",
        "    sequences=sequences,\n",
        "    context_window=2,\n",
        "    negative_sample_number=4,\n",
        "    vocab_size=vocab_size,\n",
        ")\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)[:,:,0]\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "_uBIptNIEHOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f046d466-2774-4305-c2b9-7134b94c04ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 142/511617 [00:58<71:57:08,  1.97it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 5\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "YkPuZWl8GrRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66daee96-e596-40e6-eed5-6a15d5042829"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: (((5,), (5, 5)), (5, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "LawqJvfLG9q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dcf7bba-104c-478c-f545-97a057975b60"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: (((5,), (5, 5)), (5, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "VEj95vv_Gs7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ns = 4\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                             embedding_dim,\n",
        "                                             input_length=1,\n",
        "                                             name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                              embedding_dim,\n",
        "                                              input_length=num_ns+1)\n",
        "    \n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    \n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = tf.einsum('be, bce->bc', word_emb, context_emb)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "LJxbPo9kGtwK"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "LiQ7VcSjHA6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5L_-e-52HCM0"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "m2hfAQsMHErE"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "4WSge4uQHGJI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "ecd5369f-bf2b-4a1e-8a27-25eed1a7bc5a"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "13/13 [==============================] - 3s 187ms/step - loss: 1.6090 - accuracy: 0.2154\n",
            "Epoch 2/20\n",
            "13/13 [==============================] - 2s 181ms/step - loss: 1.5845 - accuracy: 0.9692\n",
            "Epoch 3/20\n",
            "13/13 [==============================] - 2s 183ms/step - loss: 1.5605 - accuracy: 1.0000\n",
            "Epoch 4/20\n",
            "13/13 [==============================] - 2s 185ms/step - loss: 1.5316 - accuracy: 1.0000\n",
            "Epoch 5/20\n",
            "13/13 [==============================] - 2s 183ms/step - loss: 1.4948 - accuracy: 1.0000\n",
            "Epoch 6/20\n",
            " 1/13 [=>............................] - ETA: 2s - loss: 1.4509 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-199-0f8ff9848ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "Y1jExxVRHJZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize code\n",
        "\n",
        "\n",
        "*   https://sebastianraschka.com/blog/2020/numpy-intro.html#array-math-and-universal-functions\n",
        "*   https://pythonspeed.com/articles/vectorization-python/\n",
        "\n"
      ],
      "metadata": {
        "id": "igSNWgNgDw1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l_f41P9XDwDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
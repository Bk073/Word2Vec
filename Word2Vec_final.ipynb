{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOR3BdXW1eGFcjOm+AvlG8g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bk073/Word2Vec/blob/main/Word2Vec_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "5HmoXV23CN6U"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import itertools\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMzW09CAYRBF",
        "outputId": "3e78b7a9-2b1b-4c19-d864-3d333f0940b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "L50tnoVCCSNm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process data"
      ],
      "metadata": {
        "id": "4WPa9-87E-Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  import codecs\n",
        "  with codecs.open(file_path, 'r', encoding='latin1',errors='ignore') as fdata:\n",
        "    data = fdata.read()\n",
        "  return data\n",
        "        \n",
        "  # with open(file_path, 'r') as f:\n",
        "  #   for line in f:\n",
        "  #     line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
        "  #     content += line\n",
        "  # return content"
      ],
      "metadata": {
        "id": "F4orurpeCUty"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 20-news group data\n",
        "# file_path = tf.keras.utils.get_file(fname='wiki_dump', origin='http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2', extract=True, cache_subdir='/content/drive/MyDrive/word2vec')"
      ],
      "metadata": {
        "id": "T_uSz71jCct6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tarfile\n",
        "\n",
        "# t_file = tarfile.open('/content/drive/MyDrive/word2vec/20_news_group.tar.gz')\n",
        "# t_file.extractall('/content/drive/MyDrive/word2vec/20_news_group')\n",
        "# t_file.close()"
      ],
      "metadata": {
        "id": "IePdpl8JFUdE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_dump(root_path):\n",
        "  train_data_list = ''\n",
        "  for i in os.listdir(root_path):\n",
        "    f = os.path.join(root_path, i)\n",
        "    for j in os.listdir(f):\n",
        "      # train_data_list.append(os.path.join(f, j))\n",
        "      train_data_list += read_file(os.path.join(f, j))\n",
        "      # except:\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "  \n",
        "  return train_data_list"
      ],
      "metadata": {
        "id": "ltDcQo0MCf4s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dir = '/content/drive/MyDrive/word2vec/20_news_group/20news-bydate-train'\n",
        "# word_list = data_dump(train_dir)"
      ],
      "metadata": {
        "id": "-z_P2K2VCj9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/word2vec/words.txt', 'w') as f:\n",
        "#   f.write(word_list)"
      ],
      "metadata": {
        "id": "JTA9bGkRYq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_file_path =  '/content/drive/MyDrive/word2vec/words.txt'\n",
        "# test\n",
        "# raw_file_path = '/content/drive/MyDrive/word2vec/text.txt'\n",
        "with open(raw_file_path, 'r') as f:\n",
        "  raw_file = f.read()"
      ],
      "metadata": {
        "id": "7LOMc9NgYsbd"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sentence, lang):\n",
        "  # lower = sentence.lower()\n",
        "  # email_removed = tf.strings.regex_replace(\n",
        "  #     lower, \"\\S*@\\S*\\s?\", \"\"\n",
        "  # )\n",
        "  # number_removed = tf.strings.regex_replace(\n",
        "  #     email_removed, \"[0-9]\", ' '\n",
        "  # )\n",
        "  # punctuation_removed =  tf.strings.regex_replace(\n",
        "  #     number_removed, '[%s]' % re.escape(string.punctuation), ' '\n",
        "  # )\n",
        "  # multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed), separator=\" \")\n",
        "  # return multiple_space_removed\n",
        "  if lang == 'eng':\n",
        "    punctuation_ = set(string.punctuation)\n",
        "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'\\S*@\\S*\\s?', ' ', sentence)\n",
        "    sentence = re.sub(r'[0-9]', ' ', sentence)\n",
        "    # sentence = ''.join(ch for ch in sentence if ch not in punctuation_)\n",
        "    # sentence = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *', ' ', sentence)\n",
        "    # sentence = re.sub(r\"[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *\", \" \", sentence)\n",
        "    sentence = sentence.translate(translator)\n",
        "    sentence = re.sub(r'\\n', ' ', sentence)\n",
        "    sentence = re.sub(r'\\t+', ' ', sentence)\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "l2T3R9YeCoWd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocess_text(raw_file, 'eng')\n",
        "print(preprocessed_data[:100])"
      ],
      "metadata": {
        "id": "pKR5ai2-C0QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "320171b3-e694-4117-ba45-a3c6d00321d8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from mathew subject alt atheism faq atheist resources summary books addresses music anything related\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subsampling(train_data, threshold=1e-5):\n",
        "  total_words = train_data.split(' ')\n",
        "  print(f\"total words: {len(total_words)}\")\n",
        "  print(f\"unique words: {len(set(total_words))}\")\n",
        "\n",
        "  c = Counter (total_words)\n",
        "  total_counts = len(total_words)\n",
        "  freq = {word: (count / total_counts) for word, count in c.items()}\n",
        "  probability = {word: (1-np.sqrt(threshold/freq[word])) for word in c}\n",
        "\n",
        "  train_words = [word for word in total_words if 0.2 < (1 - probability[word])]\n",
        "  print(f\"subsampled vocab length: {len(train_words)}\")\n",
        "\n",
        "  return train_words, total_words\n"
      ],
      "metadata": {
        "id": "r0HFHNQ0DEv8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words, total_words = subsampling(preprocessed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p45gLUe1n3z7",
        "outputId": "5dc45777-0308-4783-fbae-e3ba2026bf31"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words: 3300978\n",
            "unique words: 77710\n",
            "subsampled vocab length: 1245470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syiRzNCDoTyC",
        "outputId": "1d2ffd41-a59d-4512-94ae-2c87098bdd14"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mathew',\n",
              " 'alt',\n",
              " 'atheism',\n",
              " 'faq',\n",
              " 'atheist',\n",
              " 'resources',\n",
              " 'summary',\n",
              " 'books',\n",
              " 'addresses',\n",
              " 'music',\n",
              " 'related',\n",
              " 'atheism',\n",
              " 'faq',\n",
              " 'atheism',\n",
              " 'books',\n",
              " 'music',\n",
              " 'fiction',\n",
              " 'addresses',\n",
              " 'contacts',\n",
              " 'expires']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.TextLineDataset(raw_file_path)"
      ],
      "metadata": {
        "id": "EQo3vS2tFr_0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_data.take(2):\n",
        "  print(text.numpy())"
      ],
      "metadata": {
        "id": "gwh2-OjCFtJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae282581-feb9-46f7-9d2b-eca7af024f33"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'From: mathew <mathew@mantis.co.uk>'\n",
            "b'Subject: Alt.Atheism FAQ: Atheist Resources'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  lower = tf.strings.lower(text)\n",
        "  # remove emails\n",
        "  email_removed = tf.strings.regex_replace(\n",
        "      lower, \"\\S*@\\S*\\s?\", \"\"\n",
        "  )\n",
        "  # remove numbers\n",
        "  number_removed = tf.strings.regex_replace(\n",
        "      email_removed, \"[0-9]\", ' '\n",
        "  )\n",
        "  tab_removed = tf.strings.regex_replace(number_removed, \"\\t+\", \" \")\n",
        "  # remove punctuations\n",
        "  punctuation_removed =  tf.strings.regex_replace(\n",
        "      tab_removed, '[%s]' % re.escape(string.punctuation), ' '\n",
        "  )\n",
        "  # remove multiple blank spaces\n",
        "  # multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed), separator=\" \")\n",
        "  multiple_space_removed = tf.strings.regex_replace(punctuation_removed, ' +', ' ')\n",
        "  return multiple_space_removed\n"
      ],
      "metadata": {
        "id": "jRe_vAsVF92n"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(subsampled_words))\n",
        "sequence_length = 10\n",
        "\n",
        "vectorize_layers = layers.TextVectorization(\n",
        "    standardize=preprocess,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "sw_mGe7UF-k_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layers.adapt(train_data.batch(1024))\n",
        "# vectorize_layers.adapt(train_words)"
      ],
      "metadata": {
        "id": "rsy6HNQpGFlR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorize_layers.vocabulary_size())\n",
        "print(vectorize_layers.get_vocabulary()[:20])"
      ],
      "metadata": {
        "id": "OoFIpVUQGHne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acd192f7-6ce1-4ac2-8ae4-07f3271d59bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77292\n",
            "['', '[UNK]', 'the', 'to', 'of', 'a', 'ax', 'and', 'i', 'in', 'is', 'that', 'it', 'for', 'you', 's', 'from', 't', 'on', 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = vectorize_layers.get_vocabulary()\n",
        "inverse_vocab[-20:]"
      ],
      "metadata": {
        "id": "js7p0SxAGb5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db52274b-6bc8-46c7-ebb6-9eca3b006196"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['affectionately',\n",
              " 'affectionate',\n",
              " 'affection',\n",
              " 'affaire',\n",
              " 'affadavit',\n",
              " 'afew',\n",
              " 'afeiqy',\n",
              " 'afeiqx',\n",
              " 'afeiqvf',\n",
              " 'afeial',\n",
              " 'afeiafei',\n",
              " 'afdzqvg',\n",
              " 'afdu',\n",
              " 'afdod',\n",
              " 'afarensis',\n",
              " 'afal',\n",
              " 'afair',\n",
              " 'afa',\n",
              " 'aether',\n",
              " 'aetana']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(inverse_vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npoI6VRTyh8b",
        "outputId": "14c45492-5344-4674-9126-f0060cce79f3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77292"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vector_ds = train_data.batch(1024).prefetch(AUTOTUNE).map(vectorize_layers).unbatch()"
      ],
      "metadata": {
        "id": "dSrrOXKPGfpg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub sampling\n",
        "for i in text_vector_ds.take(2):\n",
        "  print(f\"Vector: {i.numpy()} => word: {[inverse_vocab[x] for x in i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrslo1nIbVvo",
        "outputId": "953b4942-4bcd-4b80-a1d5-07397678bbf5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector: [  16 3558    0    0    0    0    0    0    0    0] => word: ['from', 'mathew', '', '', '', '', '', '', '', '']\n",
            "Vector: [  31 1032 1070  503 1597 1480    0    0    0    0] => word: ['subject', 'alt', 'atheism', 'faq', 'atheist', 'resources', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inverse_vocab[3558])\n",
        "\n",
        "if inverse_vocab[3558] in subsampled_words:\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbUU1jwxqZ-w",
        "outputId": "807fd955-03d7-4464-d31c-fcc582ed44da"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mathew\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list(text_vector_ds.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "metadata": {
        "id": "BtR7YOGSGidY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec3dc1d-da08-45ca-f5ef-77fdfb15aeac"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inverse_vocab[-1] in subsampled_words:\n",
        "  print(\"true\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4StMfqlwnixd",
        "outputId": "d9ebd864-ead8-4b04-b0fb-c6332c22e2a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in inverse_vocab:\n",
        "  print(x)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z54KpCjg-qgD",
        "outputId": "fed93d63-d881-4dd7-c269-b1ae825b30b8"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words = list(set(subsampled_words))\n",
        "len(list(set(subsampled_words)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ukofs4D2el5",
        "outputId": "a678229c-94f1-4bbb-e249-078f3a8291ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "77292"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## list of sub-sample but with integer number from inverse_vocab\n",
        "sub_sample_vocab_id = [inverse_vocab.index(word) for word in inverse_vocab if (word in subsampled_words)]"
      ],
      "metadata": {
        "id": "3s6kYJFs82mG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('/content/drive/MyDrive/word2vec/sub_sample_vocab_id.txt', 'wb') as f:\n",
        "  pickle.dump(sub_sample_vocab_id, f)"
      ],
      "metadata": {
        "id": "53QDMmMSKG2q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/word2vec/sub_sample_vocab_id.txt', 'rb') as f:\n",
        "  sub_sample_vocab_id = pickle.load(f)"
      ],
      "metadata": {
        "id": "1TfiAobPKZfN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating vocab"
      ],
      "metadata": {
        "id": "FFyiFmAiD1bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab = Counter(set(total_words))\n",
        "id_word = {id:word for id, word in enumerate(inverse_vocab)}\n",
        "word_id = {word:id for id, word in id_word.items()}"
      ],
      "metadata": {
        "id": "-37C2ndaDzNr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-gram with sub-sampling"
      ],
      "metadata": {
        "id": "7llhhatkEULV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def changed_skip_gram(context_window, index_vocab):\n",
        "  context_vector = []\n",
        "  # negative_vector = []\n",
        "  sub_sample_vocab_id_local = sub_sample_vocab_id\n",
        "  try:\n",
        "    padding_index = index_vocab.tolist().index(0)\n",
        "    for index, target in enumerate(index_vocab):\n",
        "      # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "      # TODO\n",
        "      # check if the target's probability < threshold\n",
        "      if (target in sub_sample_vocab_id_local):\n",
        "        if (index<padding_index):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < padding_index else padding_index\n",
        "          context_vector.extend([[target, index_vocab[c]] for c in range(left_index, right_index) if (c != index and index_vocab[c] in sub_sample_vocab_id_local)])\n",
        "        # elif index > padding_index-context_window-1 and index < padding_index:\n",
        "          # left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          # right_index = index+context_window+1 if (index+context_window+1) <= padding_index else len(padding_index)\n",
        "          # context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if c != index])\n",
        "      \n",
        "      else:\n",
        "        continue\n",
        "  except:\n",
        "    for index, target in enumerate(index_vocab):\n",
        "    # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "    # TODO\n",
        "    # Check probability i.e sub-sampling need to be done\n",
        "      if (target in sub_sample_vocab_id_local):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < len(index_vocab) else len(index_vocab)\n",
        "          context_vector.extend([[target, index_vocab[c]] for c in range(left_index, right_index) if (c != index and index_vocab[c] in sub_sample_vocab_id_local)])\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  context_vector.sort()\n",
        "  context_vector = list(k for k, _ in itertools.groupby(context_vector))\n",
        "  return context_vector\n",
        "\n",
        "  #   negative_sample = negative_sampling(target, context_vector, negative_sample_number, vocab, vocab_size)\n",
        "  #   negative_vector.extend(negative_sample)\n",
        "  # return context_vector, negative_vector"
      ],
      "metadata": {
        "id": "M2yZACGn_Jy6"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram(index_vocab, context_window):\n",
        "  context_vector = []\n",
        "  inverse_vocab_local = inverse_vocab\n",
        "  subsampled_words_local = subsampled_words\n",
        "  # negative_vector = []\n",
        "  try:\n",
        "    padding_index = index_vocab.tolist().index(0)\n",
        "    for index, target in enumerate(index_vocab):\n",
        "      # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "      # TODO\n",
        "      # check if the target's probability < threshold\n",
        "      if (inverse_vocab_local[target] in subsampled_words_local):\n",
        "        if (index<padding_index):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < padding_index else padding_index\n",
        "          context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if (c != index and inverse_vocab_local[index_vocab[c]] in subsampled_words_local)])\n",
        "        # elif index > padding_index-context_window-1 and index < padding_index:\n",
        "          # left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          # right_index = index+context_window+1 if (index+context_window+1) <= padding_index else len(padding_index)\n",
        "          # context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if c != index])\n",
        "      \n",
        "      else:\n",
        "        continue\n",
        "  except:\n",
        "    for index, target in enumerate(index_vocab):\n",
        "    # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "    # TODO\n",
        "    # Check probability i.e sub-sampling need to be done\n",
        "      if (inverse_vocab_local[target] in subsampled_words_local):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < len(index_vocab) else len(index_vocab)\n",
        "          context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if (c != index and inverse_vocab_local[index_vocab[c]] in subsampled_words_local)])\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  context_vector.sort()\n",
        "  context_vector = list(k for k, _ in itertools.groupby(context_vector))\n",
        "  return context_vector\n",
        "\n",
        "  #   negative_sample = negative_sampling(target, context_vector, negative_sample_number, vocab, vocab_size)\n",
        "  #   negative_vector.extend(negative_sample)\n",
        "  # return context_vector, negative_vector"
      ],
      "metadata": {
        "id": "zpJ9LxaMETaK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = [31, 1032, 1070, 503, 1597, 1480]\n",
        "window_size=2\n",
        "# positive_skip_grams_2 = skip_gram(example_sentence, window_size)\n",
        "positive_skip_grams_2 = changed_skip_gram(window_size, example_sentence)\n",
        "print(positive_skip_grams_2)\n",
        "print(len(positive_skip_grams_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMbMB-UlEfV3",
        "outputId": "e1c125cc-99f3-4d01-a869-1c4247394089"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[503, 1032], [503, 1070], [503, 1480], [503, 1597], [1032, 503], [1032, 1070], [1070, 503], [1070, 1032], [1070, 1597], [1480, 503], [1480, 1597], [1597, 503], [1597, 1070], [1597, 1480]]\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inverse_vocab[31] in subsampled_words:\n",
        "  print(\"True\")"
      ],
      "metadata": {
        "id": "QPd2aBXf05o4"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in example_sentence:\n",
        "  if inverse_vocab[31] in subsampled_words:\n",
        "    print(\"True\")"
      ],
      "metadata": {
        "id": "laRB1iaPs8TS"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative sampling"
      ],
      "metadata": {
        "id": "iRTCQHOnEuIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## negative sampling\n",
        "def negative_sampling(target, context, negative_sample_number, vocab_size):\n",
        "  # vocab_size = len(vocab)\n",
        "  # TODO\n",
        "# this implementation continues in loop when context_window is greater\n",
        "# suppose we have vocab size of 4 [1,2,3,4] and context_window = 3 and negative sample = 2\n",
        "# then we can not get a negative sample of size 2 and the while loop will continue forever\n",
        "# this need to be handle\n",
        "  negative_sample = []\n",
        "  # negative_sample_number = 2\n",
        "  # vocab_size = 7\n",
        "  # vocab = index_vocab\n",
        "  # or get the context for particular index and sample negative values from vocab except the context\n",
        "  while len(negative_sample) < negative_sample_number:\n",
        "    import pdb\n",
        "    # pdb.set_trace()\n",
        "    sample = random.randint(0, vocab_size-1)\n",
        "    # if inverse_vocab[sample] in subsampled_words:\n",
        "    if (sample != context) and (sample not in negative_sample) and sample != target:\n",
        "      negative_sample.append(sample)\n",
        "  return tf.convert_to_tensor(negative_sample, dtype='int64')"
      ],
      "metadata": {
        "id": "Fi3JQixXEwbg"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate training data"
      ],
      "metadata": {
        "id": "H5OuUVxHE2mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram_with_negative_sampling(sequences,context_window, negative_sample_number, vocab_size):\n",
        "  targets, contexts, labels = [],[],[]\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "  # for positive_skip_grams in tqdm.tqdm(map(functools.partial(changed_skip_gram, window_size), sequences)):\n",
        "    import pdb\n",
        "    # TODO\n",
        "    # map function all at once and loop, rather making function call each time\n",
        "    positive_skip_grams = skip_gram(sequence, context_window)\n",
        "    # positive_skip_grams = changed_skip_gram(window_size, sequence)\n",
        "    # pdb.set_trace()\n",
        "    if positive_skip_grams:\n",
        "      for target_word, context_word in positive_skip_grams:\n",
        "        context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
        "        negative_sampling_candidate = negative_sampling(target_word, context_word, negative_sample_number, vocab_size)\n",
        "\n",
        "        negative_sampling_candidate = tf.expand_dims(negative_sampling_candidate, 1)\n",
        "        context = tf.concat([context_class, negative_sampling_candidate], 0)\n",
        "        label = tf.constant([1] + [0]*negative_sample_number, dtype=\"int64\")\n",
        "\n",
        "        targets.append(target_word)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "    else:\n",
        "      continue\n",
        "  \n",
        "  return targets, contexts, labels\n"
      ],
      "metadata": {
        "id": "MpEqIvgyE1qD"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sampled training words to int\n",
        "\n",
        "training_word_int = [word_id[word] for word in train_words]\n",
        "training_word_int[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "WpRXzbnwEDPg",
        "outputId": "695e4d7d-0576-490d-dd24-6b0d8776a4a9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-c9cbbd606e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mapping sampled training words to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_word_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_word_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets, contexts, labels = skip_gram_with_negative_sampling(\n",
        "    sequences=sequences,\n",
        "    context_window=2,\n",
        "    negative_sample_number=4,\n",
        "    vocab_size=vocab_size,\n",
        ")\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)[:,:,0]\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "_uBIptNIEHOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6b452c5-2bf9-41a6-f8a4-5e68e07aa2e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 692/511617 [01:27<22:13:04,  6.39it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "YkPuZWl8GrRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41889493-cb89-46cc-8a84-21e3d37ea91f"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset shapes: (((2,), (2, 5)), (2, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "LawqJvfLG9q2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83ecdd1f-e95c-42ec-ef15-095d7a6f983d"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: (((2,), (2, 5)), (2, 5)), types: ((tf.int64, tf.int64), tf.int64)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "VEj95vv_Gs7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ns = 4\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                             embedding_dim,\n",
        "                                             input_length=1,\n",
        "                                             name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                              embedding_dim,\n",
        "                                              input_length=num_ns+1)\n",
        "    \n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    \n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = tf.einsum('be, bce->bc', word_emb, context_emb)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "LJxbPo9kGtwK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "LiQ7VcSjHA6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5L_-e-52HCM0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "m2hfAQsMHErE"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.fit(dataset, epochs=5, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "4WSge4uQHGJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "547d8938-e8ad-4867-94d4-9fec7b07440f"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "32/32 [==============================] - 8s 208ms/step - loss: 1.6089 - accuracy: 0.2188\n",
            "Epoch 2/5\n",
            "32/32 [==============================] - 6s 202ms/step - loss: 1.5723 - accuracy: 1.0000\n",
            "Epoch 3/5\n",
            "32/32 [==============================] - 7s 209ms/step - loss: 1.5280 - accuracy: 1.0000\n",
            "Epoch 4/5\n",
            "32/32 [==============================] - 7s 208ms/step - loss: 1.4618 - accuracy: 1.0000\n",
            "Epoch 5/5\n",
            "32/32 [==============================] - 6s 200ms/step - loss: 1.3647 - accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1a23eaf4d0>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "Y1jExxVRHJZF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331078be-7ea7-4310-e5b6-593c3174f71b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%tensorboard` not found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorize code\n",
        "\n",
        "\n",
        "*   https://sebastianraschka.com/blog/2020/numpy-intro.html#array-math-and-universal-functions\n",
        "*   https://pythonspeed.com/articles/vectorization-python/\n",
        "\n"
      ],
      "metadata": {
        "id": "igSNWgNgDw1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "l_f41P9XDwDH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word2Vec-final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNlOlaCoZBldkAXMsgmpK0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bk073/Word2Vec/blob/main/Word2Vec_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5HmoXV23CN6U"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import re\n",
        "import string\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import random\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMzW09CAYRBF",
        "outputId": "c491d42e-02d3-407a-ca41-17334fb7f030"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "AUTOTUNE = tf.data.AUTOTUNE"
      ],
      "metadata": {
        "id": "L50tnoVCCSNm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Process data"
      ],
      "metadata": {
        "id": "4WPa9-87E-Cm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "  import codecs\n",
        "  with codecs.open(file_path, 'r', encoding='latin1',errors='ignore') as fdata:\n",
        "    data = fdata.read()\n",
        "  return data\n",
        "        \n",
        "  # with open(file_path, 'r') as f:\n",
        "  #   for line in f:\n",
        "  #     line = bytes(line, 'utf-8').decode('utf-8', 'ignore')\n",
        "  #     content += line\n",
        "  # return content"
      ],
      "metadata": {
        "id": "F4orurpeCUty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download 20-news group data\n",
        "# file_path = tf.keras.utils.get_file(fname='wiki_dump', origin='http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2', extract=True, cache_subdir='/content/drive/MyDrive/word2vec')"
      ],
      "metadata": {
        "id": "T_uSz71jCct6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tarfile\n",
        "\n",
        "# t_file = tarfile.open('/content/drive/MyDrive/word2vec/20_news_group.tar.gz')\n",
        "# t_file.extractall('/content/drive/MyDrive/word2vec/20_news_group')\n",
        "# t_file.close()"
      ],
      "metadata": {
        "id": "IePdpl8JFUdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_dump(root_path):\n",
        "  train_data_list = ''\n",
        "  for i in os.listdir(root_path):\n",
        "    f = os.path.join(root_path, i)\n",
        "    for j in os.listdir(f):\n",
        "      # train_data_list.append(os.path.join(f, j))\n",
        "      train_data_list += read_file(os.path.join(f, j))\n",
        "      # except:\n",
        "        # import pdb\n",
        "        # pdb.set_trace()\n",
        "  \n",
        "  return train_data_list"
      ],
      "metadata": {
        "id": "ltDcQo0MCf4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dir = '/content/drive/MyDrive/word2vec/20_news_group/20news-bydate-train'\n",
        "# word_list = data_dump(train_dir)"
      ],
      "metadata": {
        "id": "-z_P2K2VCj9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/MyDrive/word2vec/words.txt', 'w') as f:\n",
        "#   f.write(word_list)"
      ],
      "metadata": {
        "id": "JTA9bGkRYq03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_file_path =  '/content/drive/MyDrive/word2vec/words.txt'\n",
        "with open(raw_file_path, 'r') as f:\n",
        "  raw_file = f.read()"
      ],
      "metadata": {
        "id": "7LOMc9NgYsbd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(sentence, lang):\n",
        "  # lower = sentence.lower()\n",
        "  # email_removed = tf.strings.regex_replace(\n",
        "  #     lower, \"\\S*@\\S*\\s?\", \"\"\n",
        "  # )\n",
        "  # number_removed = tf.strings.regex_replace(\n",
        "  #     email_removed, \"[0-9]\", ' '\n",
        "  # )\n",
        "  # punctuation_removed =  tf.strings.regex_replace(\n",
        "  #     number_removed, '[%s]' % re.escape(string.punctuation), ' '\n",
        "  # )\n",
        "  # multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed), separator=\" \")\n",
        "  # return multiple_space_removed\n",
        "  if lang == 'eng':\n",
        "    punctuation_ = set(string.punctuation)\n",
        "    translator = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "    sentence = sentence.lower()\n",
        "    sentence = re.sub(r'\\S*@\\S*\\s?', ' ', sentence)\n",
        "    sentence = re.sub(r'[0-9]', ' ', sentence)\n",
        "    # sentence = ''.join(ch for ch in sentence if ch not in punctuation_)\n",
        "    # sentence = re.sub(r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *', ' ', sentence)\n",
        "    # sentence = re.sub(r\"[!\\\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]+\\ *\", \" \", sentence)\n",
        "    sentence = sentence.translate(translator)\n",
        "    sentence = re.sub(r'\\n', ' ', sentence)\n",
        "    sentence = re.sub(r'\\t+', ' ', sentence)\n",
        "    sentence = re.sub(' +', ' ', sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n"
      ],
      "metadata": {
        "id": "l2T3R9YeCoWd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_data = preprocess_text(raw_file, 'eng')\n",
        "print(preprocessed_data[:100])"
      ],
      "metadata": {
        "id": "pKR5ai2-C0QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4517ebce-5ea9-452c-cf5e-228d56b37c88"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "from mathew subject alt atheism faq atheist resources summary books addresses music anything related\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def subsampling(train_data, threshold=1e-5):\n",
        "  total_words = train_data.split(' ')\n",
        "  print(f\"total words: {len(total_words)}\")\n",
        "  print(f\"unique words: {len(set(total_words))}\")\n",
        "\n",
        "  c = Counter (total_words)\n",
        "  total_counts = len(total_words)\n",
        "  freq = {word: (count / total_counts) for word, count in c.items()}\n",
        "  probability = {word: (1-np.sqrt(threshold/freq[word])) for word in c}\n",
        "\n",
        "  train_words = [word for word in total_words if 0.2 < (1 - probability[word])]\n",
        "  print(f\"subsampled vocab length: {len(train_words)}\")\n",
        "\n",
        "  return train_words\n"
      ],
      "metadata": {
        "id": "r0HFHNQ0DEv8"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words = subsampling(preprocessed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p45gLUe1n3z7",
        "outputId": "810f19eb-45df-4037-9f5f-6ba00ac8e6cd"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total words: 3300978\n",
            "unique words: 77710\n",
            "subsampled vocab length: 1245470\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "subsampled_words[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syiRzNCDoTyC",
        "outputId": "db485485-fa2b-4901-f4a7-468f161d8da6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['mathew',\n",
              " 'alt',\n",
              " 'atheism',\n",
              " 'faq',\n",
              " 'atheist',\n",
              " 'resources',\n",
              " 'summary',\n",
              " 'books',\n",
              " 'addresses',\n",
              " 'music',\n",
              " 'related',\n",
              " 'atheism',\n",
              " 'faq',\n",
              " 'atheism',\n",
              " 'books',\n",
              " 'music',\n",
              " 'fiction',\n",
              " 'addresses',\n",
              " 'contacts',\n",
              " 'expires']"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tf.data.TextLineDataset(raw_file_path)"
      ],
      "metadata": {
        "id": "EQo3vS2tFr_0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_data.take(2):\n",
        "  print(text.numpy())"
      ],
      "metadata": {
        "id": "gwh2-OjCFtJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad946988-0022-46bd-f2a6-35a373e8e561"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'From: mathew <mathew@mantis.co.uk>'\n",
            "b'Subject: Alt.Atheism FAQ: Atheist Resources'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  lower = tf.strings.lower(text)\n",
        "  # remove emails\n",
        "  email_removed = tf.strings.regex_replace(\n",
        "      lower, \"\\S*@\\S*\\s?\", \"\"\n",
        "  )\n",
        "  # remove numbers\n",
        "  number_removed = tf.strings.regex_replace(\n",
        "      email_removed, \"[0-9]\", ' '\n",
        "  )\n",
        "  tab_removed = tf.strings.regex_replace(number_removed, \"\\t+\", \" \")\n",
        "  # remove punctuations\n",
        "  punctuation_removed =  tf.strings.regex_replace(\n",
        "      tab_removed, '[%s]' % re.escape(string.punctuation), ' '\n",
        "  )\n",
        "  # remove multiple blank spaces\n",
        "  # multiple_space_removed =tf.strings.reduce_join(tf.strings.split(punctuation_removed), separator=\" \")\n",
        "  multiple_space_removed = tf.strings.regex_replace(punctuation_removed, ' +', ' ')\n",
        "  return multiple_space_removed\n"
      ],
      "metadata": {
        "id": "jRe_vAsVF92n"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(total_words))\n",
        "sequence_length = 10\n",
        "\n",
        "vectorize_layers = layers.TextVectorization(\n",
        "    standardize=preprocess,\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length\n",
        ")"
      ],
      "metadata": {
        "id": "sw_mGe7UF-k_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layers.adapt(train_data.batch(1024))\n",
        "# vectorize_layers.adapt(train_words)"
      ],
      "metadata": {
        "id": "rsy6HNQpGFlR"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorize_layers.vocabulary_size())\n",
        "print(vectorize_layers.get_vocabulary()[:20])"
      ],
      "metadata": {
        "id": "OoFIpVUQGHne",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef936fad-fb36-46ab-dff0-504e4ce2923c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77710\n",
            "['', '[UNK]', 'the', 'to', 'of', 'a', 'ax', 'and', 'i', 'in', 'is', 'that', 'it', 'for', 'you', 's', 'from', 't', 'on', 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inverse_vocab = vectorize_layers.get_vocabulary()\n",
        "inverse_vocab[-20:]"
      ],
      "metadata": {
        "id": "js7p0SxAGb5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd2add6-4286-4a80-d8fc-15b6dffa750f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['aangezien',\n",
              " 'aangegeven',\n",
              " 'aangeboden',\n",
              " 'aanerud',\n",
              " 'aanbieden',\n",
              " 'aams',\n",
              " 'aamazing',\n",
              " 'aam',\n",
              " 'aalternate',\n",
              " 'aah',\n",
              " 'aaef',\n",
              " 'aacc',\n",
              " 'aaahh',\n",
              " 'aaaaagggghhhh',\n",
              " 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaauuuuuuuuuuuuuuuuuuuuuuuuuuuuuuuugggggggggggggggg',\n",
              " 'aaaaaaaaaaaa',\n",
              " '\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e\\x1e',\n",
              " '\\x18',\n",
              " '\\x08s',\n",
              " '\\x08r']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_vector_ds = train_data.batch(1024).prefetch(AUTOTUNE).map(vectorize_layers).unbatch()"
      ],
      "metadata": {
        "id": "dSrrOXKPGfpg"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sub sampling\n",
        "for i in text_vector_ds.take(2):\n",
        "  print(f\"Vector: {i.numpy()} => word: {[inverse_vocab[x] for x in i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrslo1nIbVvo",
        "outputId": "02ae8bd8-5c55-4389-dc33-89f18394672e"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector: [  16 3558    0    0    0    0    0    0    0    0] => word: ['from', 'mathew', '', '', '', '', '', '', '', '']\n",
            "Vector: [  31 1032 1070  503 1597 1480    0    0    0    0] => word: ['subject', 'alt', 'atheism', 'faq', 'atheist', 'resources', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inverse_vocab[3558])\n",
        "\n",
        "if inverse_vocab[3558] in subsampled_words:\n",
        "  print(\"True\")\n",
        "else:\n",
        "  print(\"False\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbUU1jwxqZ-w",
        "outputId": "14145584-12c0-41fe-ae23-9493f554795f"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mathew\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = list(train_data.as_numpy_iterator())\n",
        "print(len(sequences))"
      ],
      "metadata": {
        "id": "BtR7YOGSGidY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da85dcbb-4300-45df-bf96-63867832e9fd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "511617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inverse_vocab[-1] in subsampled_words:\n",
        "  print(\"true\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4StMfqlwnixd",
        "outputId": "b185e272-7859-4c57-e120-521be8e34fab"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "true\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating vocab"
      ],
      "metadata": {
        "id": "FFyiFmAiD1bK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vocab = Counter(set(total_words))\n",
        "id_word = {id:word for id, word in enumerate(inverse_vocab)}\n",
        "word_id = {word:id for id, word in id_word.items()}"
      ],
      "metadata": {
        "id": "-37C2ndaDzNr"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skip-gram with sub-sampling"
      ],
      "metadata": {
        "id": "7llhhatkEULV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram(index_vocab, context_window):\n",
        "  context_vector = []\n",
        "  # negative_vector = []\n",
        "  try:\n",
        "    padding_index = index_vocab.tolist().index(0)\n",
        "    for index, target in enumerate(index_vocab):\n",
        "      # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "      # TODO\n",
        "      # check if the target's probability < threshold\n",
        "      if (index<padding_index) and (inverse_vocab[target] in subsampled_words):\n",
        "        left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "        right_index = index+context_window+1 if (index+context_window) < padding_index else padding_index\n",
        "        context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if (c != index and inverse_vocab[index_vocab[c]] in subsampled_words)])\n",
        "      # elif index > padding_index-context_window-1 and index < padding_index:\n",
        "        # left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "        # right_index = index+context_window+1 if (index+context_window+1) <= padding_index else len(padding_index)\n",
        "        # context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if c != index])\n",
        "      \n",
        "      else:\n",
        "        continue\n",
        "  except:\n",
        "    for index, target in enumerate(index_vocab):\n",
        "    # context = index_vocab[ index - context_window between index + context_window ] except index\n",
        "    # TODO\n",
        "    # Check probability i.e sub-sampling need to be done\n",
        "      if (inverse_vocab[target] in subsampled_words):\n",
        "          left_index = index-context_window if (index-context_window) >= 0 else 0\n",
        "          right_index = index+context_window+1 if (index+context_window) < len(index_vocab) else len(index_vocab)\n",
        "          context_vector.extend([[index_vocab[index], index_vocab[c]] for c in range(left_index, right_index) if (c != index and inverse_vocab[index_vocab[c]] in subsampled_words)])\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "  context_vector.sort()\n",
        "  context_vector = list(k for k, _ in itertools.groupby(context_vector))\n",
        "  return context_vector\n",
        "\n",
        "  #   negative_sample = negative_sampling(target, context_vector, negative_sample_number, vocab, vocab_size)\n",
        "  #   negative_vector.extend(negative_sample)\n",
        "  # return context_vector, negative_vector"
      ],
      "metadata": {
        "id": "zpJ9LxaMETaK"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sentence = [31, 1032, 1070, 503, 1597, 1480]\n",
        "window_size=2\n",
        "positive_skip_grams_2 = skip_gram(example_sentence, window_size)\n",
        "print(positive_skip_grams_2)\n",
        "print(len(positive_skip_grams_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMbMB-UlEfV3",
        "outputId": "341f8beb-84a5-4219-b14a-bb289e5abb77"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[503, 1032], [503, 1070], [503, 1480], [503, 1597], [1032, 503], [1032, 1070], [1070, 503], [1070, 1032], [1070, 1597], [1480, 503], [1480, 1597], [1597, 503], [1597, 1070], [1597, 1480]]\n",
            "14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if inverse_vocab[31] in subsampled_words:\n",
        "  print(\"True\")"
      ],
      "metadata": {
        "id": "QPd2aBXf05o4"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in example_sentence:\n",
        "  if inverse_vocab[31] in subsampled_words:\n",
        "    print(\"True\")"
      ],
      "metadata": {
        "id": "laRB1iaPs8TS"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative sampling"
      ],
      "metadata": {
        "id": "iRTCQHOnEuIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## negative sampling\n",
        "def negative_sampling(target, context, negative_sample_number, vocab_size):\n",
        "  # vocab_size = len(vocab)\n",
        "  # TODO\n",
        "# this implementation continues in loop when context_window is greater\n",
        "# suppose we have vocab size of 4 [1,2,3,4] and context_window = 3 and negative sample = 2\n",
        "# then we can not get a negative sample of size 2 and the while loop will continue forever\n",
        "# this need to be handle\n",
        "  negative_sample = []\n",
        "  # negative_sample_number = 2\n",
        "  # vocab_size = 7\n",
        "  # vocab = index_vocab\n",
        "  # or get the context for particular index and sample negative values from vocab except the context\n",
        "  while len(negative_sample) < negative_sample_number:\n",
        "    import pdb\n",
        "    # pdb.set_trace()\n",
        "    sample = random.randint(0, vocab_size-1)\n",
        "    if (sample != context) and (sample not in negative_sample) and sample != target:\n",
        "      negative_sample.append(sample)\n",
        "  return tf.convert_to_tensor(negative_sample, dtype='int64')"
      ],
      "metadata": {
        "id": "Fi3JQixXEwbg"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate training data"
      ],
      "metadata": {
        "id": "H5OuUVxHE2mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def skip_gram_with_negative_sampling(sequences,context_window, negative_sample_number, vocab_size):\n",
        "  targets, contexts, labels = [],[],[]\n",
        "  for sequence in tqdm.tqdm(sequences):\n",
        "    # import pdb\n",
        "    positive_skip_grams = skip_gram(sequence, context_window)\n",
        "    # pdb.set_trace()\n",
        "    if positive_skip_grams:\n",
        "      for target_word, context_word in positive_skip_grams:\n",
        "        context_class = tf.expand_dims(tf.constant([context_word], dtype='int64'), 1)\n",
        "        negative_sampling_candidate = negative_sampling(target_word, context_word, negative_sample_number, vocab_size)\n",
        "\n",
        "        negative_sampling_candidate = tf.expand_dims(negative_sampling_candidate, 1)\n",
        "        context = tf.concat([context_class, negative_sampling_candidate], 0)\n",
        "        label = tf.constant([1] + [0]*negative_sample_number, dtype=\"int64\")\n",
        "\n",
        "        targets.append(target_word)\n",
        "        contexts.append(context)\n",
        "        labels.append(label)\n",
        "  \n",
        "  return targets, contexts, labels\n"
      ],
      "metadata": {
        "id": "MpEqIvgyE1qD"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping sampled training words to int\n",
        "\n",
        "training_word_int = [word_id[word] for word in train_words]\n",
        "training_word_int[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "WpRXzbnwEDPg",
        "outputId": "7daefd47-65f0-495e-b426-39dcd1c6a147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-c9cbbd606e03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Mapping sampled training words to int\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_word_int\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_word_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "targets, contexts, labels = skip_gram_with_negative_sampling(\n",
        "    sequences=sequences,\n",
        "    context_window=2,\n",
        "    negative_sample_number=4,\n",
        "    vocab_size=vocab_size,\n",
        ")\n",
        "\n",
        "targets = np.array(targets)\n",
        "contexts = np.array(contexts)[:,:,0]\n",
        "labels = np.array(labels)\n",
        "\n",
        "print('\\n')\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"contexts.shape: {contexts.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")"
      ],
      "metadata": {
        "id": "_uBIptNIEHOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71ce9d9-4990-4064-afac-cba3e5bd9ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 2073/511617 [59:13<202:42:57,  1.43s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "BUFFER_SIZE = 10000\n",
        "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "YkPuZWl8GrRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "LawqJvfLG9q2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "VEj95vv_Gs7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_ns = 4\n",
        "class Word2Vec(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim):\n",
        "    super(Word2Vec, self).__init__()\n",
        "    self.target_embedding = layers.Embedding(vocab_size,\n",
        "                                             embedding_dim,\n",
        "                                             input_length=1,\n",
        "                                             name=\"w2v_embedding\")\n",
        "    self.context_embedding = layers.Embedding(vocab_size,\n",
        "                                              embedding_dim,\n",
        "                                              input_length=num_ns+1)\n",
        "    \n",
        "  def call(self, pair):\n",
        "    target, context = pair\n",
        "    if len(target.shape) == 2:\n",
        "      target = tf.squeeze(target, axis=1)\n",
        "    \n",
        "    word_emb = self.target_embedding(target)\n",
        "    context_emb = self.context_embedding(context)\n",
        "    dots = tf.einsum('be, bce->bc', word_emb, context_emb)\n",
        "    return dots"
      ],
      "metadata": {
        "id": "LJxbPo9kGtwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "LiQ7VcSjHA6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
        "word2vec.compile(optimizer='adam',\n",
        "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
        "                 metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "5L_-e-52HCM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
      ],
      "metadata": {
        "id": "m2hfAQsMHErE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "4WSge4uQHGJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "Y1jExxVRHJZF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}